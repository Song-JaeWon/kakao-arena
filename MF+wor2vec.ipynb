{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ethen 2020-06-22 15:18:38 \n",
      "\n",
      "CPython 3.6.8\n",
      "IPython 7.2.0\n",
      "\n",
      "numpy 1.18.0\n",
      "pandas 1.0.1\n",
      "matplotlib 3.0.2\n",
      "sklearn 0.21.3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse as sparse\n",
    "import implicit\n",
    "import itertools\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.keyedvectors import WordEmbeddingsKeyedVectors\n",
    "from subprocess import call\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from itertools import groupby\n",
    "from sklearn.utils import shuffle\n",
    "from implicit.evaluation import  *\n",
    "from implicit.als import AlternatingLeastSquares as ALS\n",
    "from implicit.bpr import BayesianPersonalizedRanking as BPR\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext watermark\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%watermark -a 'Ethen' -d -t -v -p numpy,pandas,matplotlib,sklearn\n",
    "\n",
    "# from arena_util import write_json\n",
    "# from arena_util import remove_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = pd.read_json(\"/Users/minki/pythonworkspace/kakao_melon/dataset/train.json\", encoding=\"utf-8\")\n",
    "te = pd.read_json(\"/Users/minki/pythonworkspace/kakao_melon/dataset/val.json\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_songs = tr.songs.tolist()\n",
    "tr_tags = tr.tags.tolist()\n",
    "tr_ids = tr.id.tolist()\n",
    "\n",
    "te_songs = te.songs.tolist()\n",
    "te_tags = te.tags.tolist()\n",
    "te_ids = te.id.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# song별 사용횟수\n",
    "song_ret = []\n",
    "for song in tr_songs:\n",
    "    song_ret += song\n",
    "    \n",
    "song_num = dict(Counter(song_ret))\n",
    "most_song = sorted(song_num.items(), key = lambda x: -x[1])\n",
    "top100_most_song = list(map(lambda x: x[0], most_song[:100]))\n",
    "top200_most_song = list(map(lambda x: x[0], most_song[:200]))\n",
    "set_top200_song = set(top200_most_song)\n",
    "\n",
    "# tag별 사용횟수\n",
    "tag_ret = []\n",
    "for tag in tr_tags:\n",
    "    tag_ret += tag\n",
    "    \n",
    "tag_num = dict(Counter(tag_ret))\n",
    "most_tag = sorted(tag_num.items(), key = lambda x: -x[1])\n",
    "top10_most_tag = list(map(lambda x: x[0], most_tag[:10]))\n",
    "top30_most_tag = list(map(lambda x: x[0], most_tag[:30]))\n",
    "set_top30_tag = set(top30_most_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aae0d594cc43463481e801df5c9a4764",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f0a3d8d8f584b009ba60644987f6b55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 이 부분은 나중에 MF 과정을 추가할 때 사용할 것.\n",
    "new_tr = []\n",
    "song_to_idx = {} # key : song, value : idx\n",
    "song_idx = 0\n",
    "song_minimum_number = 3\n",
    "\n",
    "# song별 id 초기화 및 추출\n",
    "for i, l in tqdm(enumerate(tr_songs)):\n",
    "    song_list = l\n",
    "    new_song_list = []\n",
    "    for song_id in song_list:\n",
    "        if song_num[song_id] > song_minimum_number:\n",
    "            new_song_list.append(song_id)\n",
    "            if song_id not in song_to_idx:\n",
    "                song_to_idx[song_id] = song_idx\n",
    "                song_idx += 1\n",
    "    new_song_list = [song_to_idx[x] for x in new_song_list]\n",
    "    new_tr.append(new_song_list)\n",
    "    \n",
    "n_songs = len(song_to_idx)\n",
    "\n",
    "# tag별 id 초기화 및 추출\n",
    "tag_to_idx = {} # key : tag, value : idx\n",
    "tag_idx = 0\n",
    "tag_minimum_number = 3\n",
    "\n",
    "for i, l in tqdm(enumerate(tr_tags)):\n",
    "    tag_list = l\n",
    "    new_tag_list = []\n",
    "    for tag_id in tag_list:\n",
    "        if tag_num[tag_id] > tag_minimum_number:\n",
    "            new_tag_list.append(tag_id)\n",
    "            if tag_id not in tag_to_idx:\n",
    "                tag_to_idx[tag_id] = n_songs + tag_idx\n",
    "                tag_idx += 1\n",
    "    new_tr[i].extend([tag_to_idx[x] for x in new_tag_list])\n",
    "    \n",
    "n_tags = len(tag_to_idx)\n",
    "\n",
    "# shuffle data\n",
    "new_tr = shuffle(new_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test데이터 역시 train 데이터와 같이 전처리\n",
    "new_te = []\n",
    "te_song_to_idx = {}\n",
    "te_tag_to_idx = {}\n",
    "\n",
    "song_idx = 0\n",
    "for i, l in enumerate(te_songs):\n",
    "    view = l\n",
    "    ret = [] \n",
    "    for item_id in view:\n",
    "        if item_id not in te_song_to_idx:\n",
    "            continue\n",
    "        ret.append(te_song_to_idx[item_id])\n",
    "    new_te.append(ret)\n",
    "\n",
    "tag_idx = 0\n",
    "for i, tags in enumerate(te_tags):\n",
    "    ret = []\n",
    "    for tag in tags:\n",
    "        if tag not in te_tag_to_idx:\n",
    "            continue\n",
    "        ret.append(tag)\n",
    "    new_te[i].extend([te_tag_to_idx[x] for x in ret])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw data와 매칭시키기 위해 dictionary에 저장\n",
    "# y= song, x = id\n",
    "idx_to_song = {x:y for(y,x) in song_to_idx.items()}\n",
    "idx_to_tag = {(x - n_songs):y for(y,x) in tag_to_idx.items()}\n",
    "\n",
    "# make csr_matrix\n",
    "def lil_to_csr_matrix(lil, shape = None):\n",
    "    row = []\n",
    "    col = []\n",
    "    for row_idx,list_element in enumerate(lil):\n",
    "        for j, col_idx in enumerate(list_element):\n",
    "            row.append(row_idx)\n",
    "            col.append(col_idx)\n",
    "    \n",
    "    data = np.repeat(1, sum([len(x) for x in lil]))\n",
    "    \n",
    "    return csr_matrix((data, (row,col)), shape = shape)\n",
    "\n",
    "# train csr matrix\n",
    "tr_csr_mat = lil_to_csr_matrix(new_tr, (len(new_tr), n_songs + n_tags))\n",
    "\n",
    "# test csr matrix\n",
    "te_csr_mat = lil_to_csr_matrix(new_te, (len(new_te), n_songs + n_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8afaed3dfafe412d9e00d16de1d56441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=15), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ALS 모델 생성\n",
    "# ALS 모델 생성\n",
    "import implicit\n",
    "als_model = ALS(factors=128, regularization=0.08, calculate_training_loss = True)\n",
    "als_model.fit(tr_csr_mat.T * 15.0)\n",
    "\n",
    "song_model = ALS(use_gpu=False)\n",
    "tag_model = ALS(use_gpu=False)\n",
    "\n",
    "# item_factor = Array of latent factors for each item in the training set\n",
    "# user_factor = Array of latent factors for each user in the training set\n",
    "\n",
    "song_model.user_factors = als_model.user_factors\n",
    "tag_model.user_factors = als_model.user_factors\n",
    "\n",
    "song_model.item_factors = als_model.item_factors[:n_songs]\n",
    "tag_model.item_factors = als_model.item_factors[n_songs:]\n",
    "\n",
    "song_rec_csr = tr_csr_mat[:, :n_songs]\n",
    "tag_rec_csr = tr_csr_mat[:, n_songs:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1add1d58805a4f789a2f4a18e6f1781d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=115071), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ALS를 통한 추천\n",
    "song_ret = []\n",
    "tag_ret = []\n",
    "total = []\n",
    "from tqdm.auto import tqdm\n",
    "for plylist_id in tqdm(range(tr_csr_mat.shape[0])):\n",
    "    song_rec = song_model.recommend(plylist_id, song_rec_csr, N=500)\n",
    "    song_rec = [idx_to_song[x[0]] for x in song_rec]\n",
    "    \n",
    "    tag_rec = tag_model.recommend(plylist_id, tag_rec_csr, N=50)\n",
    "    tag_rec = [idx_to_tag[x[0]] for x in tag_rec if x[0] in idx_to_tag]\n",
    "    \n",
    "    song_ret.append(song_rec)\n",
    "    tag_ret.append(tag_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "returnval = []\n",
    "for _id, rec, tag_rec in zip(tr_ids, song_ret, tag_ret):\n",
    "    returnval.append({\n",
    "        \"id\": _id,\n",
    "        \"songs\": rec[:500],\n",
    "        \"tags\": tag_rec[:50]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de66b19d5b1e4c7c9c176bde1f330f34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=115071), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "total = []\n",
    "\n",
    "for i in tqdm(range(len(returnval))):\n",
    "    sub_list = list(map(lambda x: str(x), returnval[i]['songs'])) + returnval[i]['tags']\n",
    "    total.append(sub_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-8d413a246e88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mw2v_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, max_final_vocab)\u001b[0m\n\u001b[1;32m    598\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m             seed=seed, hs=hs, negative=negative, cbow_mean=cbow_mean, min_alpha=min_alpha, compute_loss=compute_loss)\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m     def _do_train_epoch(self, corpus_file, thread_id, offset, cython_vocab, thread_private_mem, cur_epoch,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, ns_exponent, cbow_mean, min_alpha, compute_loss, **kwargs)\u001b[0m\n\u001b[1;32m    747\u001b[0m                 \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m                 \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_total_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 749\u001b[0;31m                 end_alpha=self.min_alpha, compute_loss=compute_loss)\n\u001b[0m\u001b[1;32m    750\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrim_rule\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_sentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1065\u001b[0m             \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m             \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1067\u001b[0;31m             **kwargs)\n\u001b[0m\u001b[1;32m   1068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_job_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_iterable, corpus_file, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch(\n\u001b[1;32m    551\u001b[0m                     \u001b[0mdata_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                     total_words=total_words, queue_factor=queue_factor, report_delay=report_delay)\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch_corpusfile(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay)\u001b[0m\n\u001b[1;32m    486\u001b[0m         trained_word_count, raw_word_count, job_tally = self._log_epoch_progress(\n\u001b[1;32m    487\u001b[0m             \u001b[0mprogress_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m             report_delay=report_delay, is_corpus_file_mode=False)\n\u001b[0m\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrained_word_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_word_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_tally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_log_epoch_progress\u001b[0;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m             \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprogress_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocks if workers too slow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreport\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# a thread reporting that it finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m                 \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Make Word2Vec Model\n",
    "\n",
    "min_count = 3\n",
    "size = 100\n",
    "window = 210\n",
    "sg = 5\n",
    "\n",
    "w2v_model = Word2Vec(total, min_count = min_count, size = size, window = window, sg = sg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_ply_song = {}\n",
    "te_ply_tag = {}\n",
    "total_te_ply = {}\n",
    "\n",
    "for idx, val in tqdm(te.iterrows()):\n",
    "    te_ply_song[str(val.id)] = val.songs\n",
    "    te_ply_tag[str(val.id)] = val.tags\n",
    "    val.songs = list(map(lambda x: str(x), val.songs))\n",
    "    total_te_ply[str(val.id)] = val.songs + val.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23015/23015 [00:01<00:00, 13699.57it/s]\n"
     ]
    }
   ],
   "source": [
    "# Update Model\n",
    "songs_list = song_num.keys()\n",
    "tags_list = tag_num.keys()\n",
    "total_key = list(total_te_ply.keys())\n",
    "total_list = list(total_te_ply.values())\n",
    "p2v_model = WordEmbeddingsKeyedVectors(size)\n",
    "\n",
    "ID = []\n",
    "vec = []\n",
    "idx = 0\n",
    "\n",
    "for q in tqdm(total_list):\n",
    "    tmp_vec = 0\n",
    "    if len(q) >= 1:\n",
    "        for song in q:\n",
    "            try:\n",
    "                tmp_vec += w2v_model.wv.get_vector(song)\n",
    "            except KeyError:\n",
    "                pass\n",
    "    if type(tmp_vec) != int:\n",
    "        ID.append(total_key[idx])\n",
    "        vec.append(tmp_vec)\n",
    "    idx += 1\n",
    "p2v_model.add(ID, vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23015/23015 [01:13<00:00, 311.95it/s]\n"
     ]
    }
   ],
   "source": [
    "# get_result\n",
    "no_idx = 0\n",
    "\n",
    "answer = []\n",
    "for ply_id in tqdm(total_key):\n",
    "    get_song = []\n",
    "    get_tag = []\n",
    "    try:\n",
    "        most_id = [x[0] for x in p2v_model.most_similar(str(ply_id), topn=200)]\n",
    "        for ID in most_id:\n",
    "            get_song += te_ply_song[ID]\n",
    "            get_tag += te_ply_tag[ID]\n",
    "\n",
    "        # extract top 100 songs\n",
    "        top_song = dict(Counter(get_song))\n",
    "        top_song = sorted(top_song.items(), key = lambda x: -x[1])\n",
    "        top_song = list(map(lambda x: x[0], top_song[:100]))\n",
    "        \n",
    "        # extract top 100 tags\n",
    "        top_tag = dict(Counter(get_tag))\n",
    "        top_tag = sorted(top_tag.items(), key = lambda x: -x[1])\n",
    "        top_tag = list(map(lambda x: x[0], top_tag[:10]))\n",
    "        \n",
    "        if len(top_song) != 100 or len(top_tag) != 10:\n",
    "            if len(top_song) != 100:\n",
    "                set_top_song = set(top_song)\n",
    "                empty_song_num = len(top_song)\n",
    "                dif_song = set_top200_song - set_top_song\n",
    "                top_song = top_song + dif_song[:(100 - empty_song_num)]\n",
    "            elif len(top_tag) != 10:\n",
    "                set_top_tag = set(top_tag)\n",
    "                empty_tag_num = len(top_tag)\n",
    "                dif_tag = set_top30_tag - set_top_tag\n",
    "                top_tag = top_tag + dif_tag[:(100 - empty_tag_num)]\n",
    "                \n",
    "            answer.append({\n",
    "            \"id\": int(ply_id),\n",
    "            \"songs\": top_song,\n",
    "            \"tags\": top_tag\n",
    "            })\n",
    "            \n",
    "        else :\n",
    "            answer.append({\n",
    "            \"id\": int(ply_id),\n",
    "            \"songs\": top_song,\n",
    "            \"tags\": top_tag\n",
    "            })\n",
    "        \n",
    "    except:\n",
    "        answer.append({\n",
    "            \"id\": int(ply_id),\n",
    "            \"songs\": top100_most_song,\n",
    "            \"tags\": top10_most_tag\n",
    "        })\n",
    "        no_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('results.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(answer, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
